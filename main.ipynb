{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"gpuType":"T4","provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11957816,"sourceType":"datasetVersion","datasetId":7518490}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install requirements","metadata":{}},{"cell_type":"code","source":"!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n!pip install torch_geometric","metadata":{"id":"xSkgt1zf-raF","outputId":"59f4a52f-5eb4-41e5-9fba-07432989fe78","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:05:54.639129Z","iopub.execute_input":"2025-05-28T13:05:54.639410Z","iopub.status.idle":"2025-05-28T13:07:59.217720Z","shell.execute_reply.started":"2025-05-28T13:05:54.639391Z","shell.execute_reply":"2025-05-28T13:07:59.216960Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Clone Git Repository","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/matzamp/DL_Hackaton.git\n%cd DL_Hackaton/","metadata":{"id":"5oR2D2Us-xSQ","outputId":"7086cadf-a7fe-4d75-f271-6339bee8164d","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T14:01:02.324765Z","iopub.execute_input":"2025-05-28T14:01:02.325023Z","iopub.status.idle":"2025-05-28T14:01:08.637796Z","shell.execute_reply.started":"2025-05-28T14:01:02.325003Z","shell.execute_reply":"2025-05-28T14:01:08.636888Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Download dataset from drive","metadata":{}},{"cell_type":"code","source":"!gdown --folder https://drive.google.com/drive/folders/1Z-1JkPJ6q4C6jX4brvq1VRbJH5RPUCAk -O datasets","metadata":{"id":"PxBvwB0_6xI8","outputId":"5933387c-2cfb-474f-d842-f36a3e2d2a73","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T10:05:27.924630Z","iopub.execute_input":"2025-05-28T10:05:27.924863Z","iopub.status.idle":"2025-05-28T10:05:27.937962Z","shell.execute_reply.started":"2025-05-28T10:05:27.924841Z","shell.execute_reply":"2025-05-28T10:05:27.937362Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport logging\nfrom tqdm import tqdm\nfrom torch_geometric.loader import DataLoader\nfrom torch.utils.data import random_split\nimport torch.nn as nn\n# Load utility functions from cloned repository\nfrom source.loadData import GraphDataset\nfrom source.utils import *\nfrom source.models import EdgeCentricGNNImproved, GINEClassifier\nfrom source.noisy_loss import NoisyCrossEntropyLoss\nimport torch_scatter\nimport argparse\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.metrics import f1_score\n\n# Set the random seed\nset_seed()","metadata":{"id":"lAQuCuIoBbq5","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:09:09.723892Z","iopub.execute_input":"2025-05-28T13:09:09.724392Z","iopub.status.idle":"2025-05-28T13:09:09.730826Z","shell.execute_reply.started":"2025-05-28T13:09:09.724367Z","shell.execute_reply":"2025-05-28T13:09:09.730120Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameters Setup","metadata":{}},{"cell_type":"code","source":"# hyperparameter\ndevice = 1\ndropout = 0.3\nbatch_size = 32\nepochs = 150\nlr = 0.001\nnode_dim = 1 \nhidden_dim = 64\nedge_dim = 7\nnum_classes = 6\nnum_gnn_blocks=1\nnum_gine_layers_per_block=1\nuse_dense_skip=True\nuse_global_transformer=False\nnum_transformer_layers=1\nnum_transformer_heads=4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:09:12.614028Z","iopub.execute_input":"2025-05-28T13:09:12.614879Z","iopub.status.idle":"2025-05-28T13:09:12.620770Z","shell.execute_reply.started":"2025-05-28T13:09:12.614839Z","shell.execute_reply":"2025-05-28T13:09:12.619732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Get training/test paths","metadata":{}},{"cell_type":"code","source":"args = get_arguments()\npopulate_args(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:33:01.791769Z","iopub.execute_input":"2025-05-28T13:33:01.792072Z","iopub.status.idle":"2025-05-28T13:33:21.286272Z","shell.execute_reply.started":"2025-05-28T13:33:01.792051Z","shell.execute_reply":"2025-05-28T13:33:21.285652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if args.train_path != None:\n    is_zipped_train = is_gzipped_folder(args.train_path)\nis_zipped_test = is_gzipped_folder(args.test_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:33:24.156674Z","iopub.execute_input":"2025-05-28T13:33:24.157399Z","iopub.status.idle":"2025-05-28T13:33:24.181145Z","shell.execute_reply.started":"2025-05-28T13:33:24.157373Z","shell.execute_reply":"2025-05-28T13:33:24.180403Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load train data","metadata":{}},{"cell_type":"code","source":"if args.train_path:\n    full_dataset = GraphDataset(args.train_path, transform=add_ones, is_zipped = is_zipped_train)\n    val_size = int(0.2 * len(full_dataset))\n    train_size = len(full_dataset) - val_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:33:27.432316Z","iopub.execute_input":"2025-05-28T13:33:27.432606Z","iopub.status.idle":"2025-05-28T13:33:27.436900Z","shell.execute_reply.started":"2025-05-28T13:33:27.432585Z","shell.execute_reply":"2025-05-28T13:33:27.435982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setup folders for logs and checkpoints","metadata":{}},{"cell_type":"code","source":"script_dir = os.getcwd() \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"lHX55XGECXBr","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:33:33.421867Z","iopub.execute_input":"2025-05-28T13:33:33.422606Z","iopub.status.idle":"2025-05-28T13:33:33.426391Z","shell.execute_reply.started":"2025-05-28T13:33:33.422576Z","shell.execute_reply":"2025-05-28T13:33:33.425631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Determine dataset_id\n# Prioritize train_path for ID extraction if available (typical for training runs)\n# Otherwise, use test_path (e.g., for evaluation-only runs)\npath_for_id_extraction = args.train_path if args.train_path else args.test_path\nif not path_for_id_extraction:\n    print(\"CRITICAL ERROR: Neither train_path nor test_path is defined. Cannot determine dataset_id.\")\n    dataset_id = \"error_no_path\" # This should ideally not happen\nelse:\n    dataset_id = get_dataset_id_from_path(path_for_id_extraction)\n\nprint(f\"Determined Dataset ID: {dataset_id} (derived from path: {path_for_id_extraction})\")\nif dataset_id.startswith(\"unknown\") or dataset_id == \"error_no_path\":\n    print(f\"CRITICAL WARNING: Dataset ID extraction failed or yielded an issue: '{dataset_id}'. Insert manually the dataset ID.\")\n    args_dataset_id = get_dataset_type()\n    populate_args(args_dataset_id)\n    dataset_id = args_dataset_id.dataset_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:33:35.104162Z","iopub.execute_input":"2025-05-28T13:33:35.104902Z","iopub.status.idle":"2025-05-28T13:33:35.110418Z","shell.execute_reply.started":"2025-05-28T13:33:35.104878Z","shell.execute_reply":"2025-05-28T13:33:35.109581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logs_folder = os.path.join(script_dir, \"logs\", dataset_id)\nlog_file = os.path.join(logs_folder, \"training.log\")\nos.makedirs(logs_folder, exist_ok=True)\nos.makedirs(os.path.dirname(log_file), exist_ok=True)\n\n# Get the root logger\nlogger = logging.getLogger()\n\n# Remove all existing handlers from the root logger\n# This ensures a clean setup if the cell is re-executed\nfor handler in logger.handlers[:]:\n    logger.removeHandler(handler)\n    handler.close() # It's good practice to close handlers when removing\n\n# Now, configure logging as before\nlogging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s', filemode='a')\nlogger.addHandler(logging.StreamHandler()) # This will now add only one StreamHandler\n\ncheckpoints_folder = os.path.join(script_dir, \"checkpoints\")\nos.makedirs(checkpoints_folder, exist_ok=True)","metadata":{"id":"BTYT5jYuChPb","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:33:37.876721Z","iopub.execute_input":"2025-05-28T13:33:37.877247Z","iopub.status.idle":"2025-05-28T13:33:37.884029Z","shell.execute_reply.started":"2025-05-28T13:33:37.877223Z","shell.execute_reply":"2025-05-28T13:33:37.883235Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Choose noise and model based on the dataset \n\nFor the noise if the dataset is A or C we set it to 0.2\nelse (B or D) 0.4\n\nThe light Model is used for A and B, \nThe more \"complex\" model is used for C and D","metadata":{}},{"cell_type":"code","source":"p_noisy = 0.2 if dataset_id in [\"A\", \"C\"] else 0.4\nuse_lightModel = True if dataset_id in [\"A\", \"B\"] else False\ncriterion_robust = NoisyCrossEntropyLoss(p_noisy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:33:41.112600Z","iopub.execute_input":"2025-05-28T13:33:41.113261Z","iopub.status.idle":"2025-05-28T13:33:41.117263Z","shell.execute_reply.started":"2025-05-28T13:33:41.113240Z","shell.execute_reply":"2025-05-28T13:33:41.116692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train and Evaluation functions","metadata":{}},{"cell_type":"code","source":"def train(data_loader, model, optimizer,\n                          criterion, device, checkpoint_path_base, epoch,\n                          save_intermediate_checkpoints_at_epoch=None):\n\n    model.train()\n\n    correct, total_loss, total_samples = 0, 0, 0\n    all_preds, all_labels = [], []\n    \n    for data in tqdm(data_loader, desc=f\"Epoch {epoch+1} Training\", unit=\"batch\"):\n        data = data.to(device)\n        \n        optimizer.zero_grad()\n\n        out = model(data)\n\n        loss = criterion(out, data.y)\n        total_loss += loss.item()\n\n        loss.backward()\n    \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n    \n        # Compute accuracy\n        preds = out.argmax(dim=1)\n        \n        correct += (preds == data.y).sum().item()\n        total_samples += data.y.size(0)\n        \n        # Store for F1 calculation\n        all_labels.extend(data.y.cpu().numpy())\n        all_preds.extend(preds.cpu().numpy())\n            \n    \n    # Calculate metrics\n    avg_loss = total_loss / len(data_loader) if len(data_loader) > 0 else 0\n    \n    acc = correct / total_samples if total_samples > 0 else 0\n    \n    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n\n    if save_intermediate_checkpoints_at_epoch and epoch + 1 == save_intermediate_checkpoints_at_epoch:\n        torch.save(model.state_dict(), f\"{checkpoint_path_base}_epoch{epoch+1}.pth\")\n        print(f\"Saved intermediate checkpoints for epoch {epoch+1}\")\n    \n    return avg_loss, acc, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:33:43.265566Z","iopub.execute_input":"2025-05-28T13:33:43.266172Z","iopub.status.idle":"2025-05-28T13:33:43.275989Z","shell.execute_reply.started":"2025-05-28T13:33:43.266145Z","shell.execute_reply":"2025-05-28T13:33:43.275232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(data_loader, model, device, criterion_eval, calculate_metrics=False):\n    model.eval()\n    \n    all_preds, all_labels = [], []\n    total_loss = 0.0 \n    correct = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for data in tqdm(data_loader, desc=\"Evaluating\", unit=\"batch\"):\n            data = data.to(device)\n            \n            out = model(data)\n\n            pred = out.argmax(dim=1)\n            \n            if calculate_metrics:\n                all_labels.extend(data.y.cpu().numpy())\n                all_preds.extend(pred.cpu().numpy())\n                \n                correct += (pred == data.y).sum().item()\n                total_samples += data.y.size(0)\n                \n                batch_losses = criterion_eval(out, data.y) \n                \n                # If batch_losses is unexpectedly scalar (e.g. if criterion_eval somehow still has reduction='mean')\n                if batch_losses.ndim == 0: # batch_losses is scalar\n                    total_loss += batch_losses.item() * data.y.size(0) # Estimate batch sum\n                else: # batch_losses is 1D tensor of per-sample losses\n                    total_loss += batch_losses.sum().item()\n                \n            else: \n                all_preds.extend(pred.cpu().numpy())\n\n    if calculate_metrics:\n        avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n        accuracy = correct / total_samples if total_samples > 0 else 0.0\n        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0) if total_samples > 0 else 0.0\n        return avg_loss, accuracy, f1\n    \n    return all_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:58:51.407517Z","iopub.execute_input":"2025-05-28T12:58:51.407806Z","iopub.status.idle":"2025-05-28T12:58:51.415053Z","shell.execute_reply.started":"2025-05-28T12:58:51.407785Z","shell.execute_reply":"2025-05-28T12:58:51.414354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Initialize weights properly\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:13:05.828925Z","iopub.execute_input":"2025-05-28T13:13:05.829205Z","iopub.status.idle":"2025-05-28T13:13:05.833378Z","shell.execute_reply.started":"2025-05-28T13:13:05.829183Z","shell.execute_reply":"2025-05-28T13:13:05.832728Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Instantiate the model and main train loop (if train path is provided)","metadata":{}},{"cell_type":"code","source":"if use_lightModel:\n    model = GINEClassifier(\n        node_feat_dim=node_dim,\n        edge_feat_dim=edge_dim,\n        hidden_dim=hidden_dim,\n        num_classes=num_classes,\n        num_gnn_blocks=num_gnn_blocks,\n        num_gine_layers_per_block=num_gine_layers_per_block,\n        gnn_dropout=dropout,\n        use_dense_skip=use_dense_skip,\n        use_global_transformer=use_global_transformer,\n        num_transformer_layers=num_transformer_layers,\n        num_transformer_heads=num_transformer_heads,\n        transformer_dropout=dropout,\n        classifier_dropout=dropout\n    ).to(device)\nelse:\n    model = EdgeCentricGNNImproved(\n        node_dim=node_dim,\n        hidden_dim=hidden_dim,\n        output_dim=num_classes,\n        edge_dim=edge_dim,\n        dropout=dropout\n    ).to(device)\n\nif args.train_path:\n    generator = torch.Generator().manual_seed(12)\n    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    optimizer = torch.optim.Adam(\n        model.parameters(), \n        lr=lr,\n        weight_decay=1e-4,\n        eps=1e-8\n    )\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, verbose=True, min_lr=1e-6)\n\n    model.apply(init_weights)\n    \n    num_epochs_to_run = epochs # Use the hyperparameter\n    best_f1_accuracy = 0.0   \n\n    train_losses_hist, train_accs_hist, train_f1s_hist = [], [], []\n    val_losses_hist, val_accs_hist, val_f1s_hist = [], [], []\n\n    intermediate_checkpoint_base = os.path.join(checkpoints_folder, f\"model_{dataset_id}\")\n    \n    save_intermediate_epochs = []\n    \n    logging.info(f\"Will save intermediate checkpoints at epochs: {save_intermediate_epochs}\")\n\n    for epoch_idx in range(num_epochs_to_run):\n      \n        save_at_this_epoch = epoch_idx + 1 if (epoch_idx + 1) in save_intermediate_epochs else None\n\n        train_loss, train_acc, train_f1 = train(\n            train_loader, model, optimizer,\n            criterion_robust, device,\n            intermediate_checkpoint_base,\n            epoch_idx,\n            save_intermediate_checkpoints_at_epoch=save_at_this_epoch\n        )\n\n        val_loss, val_acc, val_f1 = evaluate(val_loader, model, device, criterion_robust, calculate_metrics=True)\n\n        scheduler.step(val_f1)\n        \n        \n        train_losses_hist.append(train_loss)\n        train_accs_hist.append(train_acc)\n        train_f1s_hist.append(train_f1)\n\n        val_losses_hist.append(val_loss)\n        val_accs_hist.append(val_acc)\n        val_f1s_hist.append(val_f1)\n\n        # --- Main logging (every epoch) ---\n        log_msg = (f\"Epoch {epoch_idx + 1}/{num_epochs_to_run} | \"\n                   f\"Train Loss: {train_loss:.4f} | \"\n                   f\"Train Acc: {train_acc:.4f} | \"\n                   f\"Train F1: {train_f1:.4f} | \"\n                   f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n        # REMOVE direct print if you fixed duplicated logging: print(log_msg) \n        logging.info(log_msg) # This goes to main training.log and console\n        \n        # --- 10-Epoch Summary Logging ---\n        current_epoch_num = epoch_idx + 1\n        if current_epoch_num % 10 == 0 or current_epoch_num == num_epochs_to_run:\n            if current_epoch_num == num_epochs_to_run and current_epoch_num % 10 != 0:\n                # Handle the last segment if it's not a full 10 epochs\n                range_start = ( (current_epoch_num -1) // 10) * 10 + 1\n                range_end = current_epoch_num\n            else: # For multiples of 10, or if the last epoch is a multiple of 10\n                range_end = current_epoch_num\n                range_start = range_end - 9\n\n            # logs_folder is defined in Cell 15 (e.g., logs/A/)\n            interval_log_filename = f\"epoch_{range_start:03d}-{range_end:03d}_summary.log\"\n            interval_log_filepath = os.path.join(logs_folder, interval_log_filename)\n\n            summary_log_msg = (f\"SUMMARY for Epoch {current_epoch_num} (Interval {range_start}-{range_end}) | \"\n                               f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | \"\n                               f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n\n            try:\n                with open(interval_log_filepath, 'w') as f_interval_summary: # Use 'w' to overwrite/create\n                    import datetime # Make sure datetime is imported if not globally available\n                    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')[:-3]\n                    f_interval_summary.write(f\"{timestamp} - {summary_log_msg}\\n\")\n                logging.info(f\"Saved 10-epoch interval summary to: {interval_log_filepath}\")\n            except Exception as e:\n                logging.error(f\"Failed to write to interval summary log {interval_log_filepath}: {e}\")\n        \n        if val_f1 > best_f1_accuracy:\n            best_f1_accuracy = val_f1\n            best_model_path = os.path.join(checkpoints_folder, f\"model_{dataset_id}_epoch_{epoch_idx}.pth\")\n            torch.save(model.state_dict(), best_model_path)\n            best_save_msg = f\"Best models (val_f1: {best_f1_accuracy:.4f}) saved to {best_model_path}\"\n            # REMOVE direct print if you fixed duplicated logging: print(best_save_msg)\n            logging.info(best_save_msg)\n\n    plot_output_dir = os.path.join(logs_folder, \"plots\") # logs_folder is dataset_id specific\n    os.makedirs(plot_output_dir, exist_ok=True)\n    \n    plot_training_progress(train_losses_hist, train_accs_hist, train_f1s_hist,\n                           val_losses_hist, val_accs_hist, val_f1s_hist,\n                           plot_output_dir, \n                           plot_title_prefix=f\"Dataset {dataset_id} Combined Training\") # Updated title\n    \nelse:\n    logging.info(\"No training path provided. Skipping training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:33:49.033509Z","iopub.execute_input":"2025-05-28T13:33:49.033793Z","iopub.status.idle":"2025-05-28T13:33:49.075171Z","shell.execute_reply.started":"2025-05-28T13:33:49.033773Z","shell.execute_reply":"2025-05-28T13:33:49.074625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference function","metadata":{}},{"cell_type":"code","source":"def inference(data_loader, model, device):\n    model.eval()\n    correct = 0\n    total = 0\n    predictions = []\n    total_loss = 0\n    with torch.no_grad():\n        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n            data = data.to(device)\n            out = model(data)\n            pred = out.argmax(dim=1)\n            predictions.extend(pred.cpu().numpy())\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:34:02.998182Z","iopub.execute_input":"2025-05-28T13:34:02.998989Z","iopub.status.idle":"2025-05-28T13:34:03.004585Z","shell.execute_reply.started":"2025-05-28T13:34:02.998962Z","shell.execute_reply":"2025-05-28T13:34:03.003752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Garbage collector","metadata":{}},{"cell_type":"code","source":"import gc\ntry:\n    del train_dataset\n    del train_loader\n    del full_dataset\n    del val_dataset\n    del val_loader\nexcept:\n    print(\"Error: skipping del\")\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:34:06.394334Z","iopub.execute_input":"2025-05-28T13:34:06.394618Z","iopub.status.idle":"2025-05-28T13:34:06.596934Z","shell.execute_reply.started":"2025-05-28T13:34:06.394599Z","shell.execute_reply":"2025-05-28T13:34:06.596138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load test data, best model and predict the test data","metadata":{}},{"cell_type":"code","source":"test_dataset = GraphDataset(args.test_path, transform=add_ones, is_zipped = is_zipped_test)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"xsXZIj4Mdu3I","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:34:09.050718Z","iopub.execute_input":"2025-05-28T13:34:09.051452Z","iopub.status.idle":"2025-05-28T13:34:54.772898Z","shell.execute_reply.started":"2025-05-28T13:34:09.051426Z","shell.execute_reply":"2025-05-28T13:34:54.772139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if dataset_id == \"A\":\n    best_model_path = \"./checkpoints/model_A_epoch_132.pth\"\nelif dataset_id == \"B\":\n    best_model_path = \"./checkpoints/model_B_epoch_77.pth\"\nelif dataset_id == \"C\":\n    best_model_path = \"./checkpoints/model_C_epoch_57.pth\"\nelif dataset_id == \"D\":\n    best_model_path = \"./checkpoints/model_D_epoch_73.pth\"\nelse:\n    print(\"Error: dataset_id not defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:34:56.995521Z","iopub.execute_input":"2025-05-28T13:34:56.995807Z","iopub.status.idle":"2025-05-28T13:34:57.000064Z","shell.execute_reply.started":"2025-05-28T13:34:56.995787Z","shell.execute_reply":"2025-05-28T13:34:56.999319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(best_model_path))\npredictions = inference(test_loader, model, device)\nsave_predictions(predictions, args.test_path, dataset_id)","metadata":{"id":"x1OnGq_nCmTr","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:35:03.546623Z","iopub.execute_input":"2025-05-28T13:35:03.546923Z","iopub.status.idle":"2025-05-28T13:35:06.121838Z","shell.execute_reply.started":"2025-05-28T13:35:03.546903Z","shell.execute_reply":"2025-05-28T13:35:06.121077Z"}},"outputs":[],"execution_count":null}]}